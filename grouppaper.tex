\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[document]{ragged2e}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\usepackage{fancyhdr} % Custom headers and footers

\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{project for CS388P: Parallel Algorithms (Fall 2016)} 
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Team Write-up:\\ Prioritizing Processor Efficiency Over Time Efficiency \\ % Write your project title here
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Matthew Hudson \hspace{5mm} Walter Xia }	%Write the name of all your team members here

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------
\justify
\textbf{Abstract}

\begin{abstract}
Our paper is \textit{Time-Work Tradeoffs for Parallel Algorithms}, \textit{Spencer\cite{S97}}. In this paper, Spencer focuses on several open parallel problems that are affected by the transitive closure bottleneck. For each of the below problems, Spencer describes an algorithm that comes closer to work efficiency than existing approaches that utilize transitive closure. However, this increased work efficiency comes at the cost of an asymptotic increase in the span of each algorithm relative to the time optimal solution.  

\underline{Problems}
\begin{itemize}
\item Solving Triangular Systems of Linear Equations
\item Topological Sort
\item Breadth-First Search in Directed Graphs
\item Strongly Connected Components
\item Single Source Shortest Path
\end{itemize}

Rather than using a matrix based approach, Spencer utilizes a new data structure called \textit{nearby lists} that serves as the work horse for the subsequent algorithms. \\
\end{abstract}

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

\textit{Spencer\cite{S97}} focuses on problems for which we known of no work-time efficient parallel algorithms that provide a solution. That is, there are no known parallel algorithms that are able to solve any of the following problems within a logarithmic factor of the fastest sequential algorithm's work, where work is the product of the running time and the processor count. For the following problems, which were presented in \textit{Karp-Ramachandran\cite{KR90}}, \textit{Spencer\cite{S97}} develops parallel algorithms with a better work efficiency than transitive closure based methods: 

\begin{itemize}
\item Directed Spanning Tree
\item Breadth-First Search
\item Topological Sort
\item Cycle Detection for Directed Graphs
\item Strong Connected Components
\item Single Source Shortest Paths with Non-Negative Edges
\end{itemize}

Spencer's motivation for prioritizing work efficiency over time efficiency stems from the fact that existing parallel transitive closure algorithms for these problems perform significantly worse than their sequential counterparts when run on only a single processor. In an effort to reduce this problem, Spencer's algorithms exhibit a time-work trade-off, that is, the longer they are allowed to run, the less overall work they perform. This phenomenon exists due of the fact that certain speculative computations can be eliminated as time progresses, thus reducing overall work. The first full algorithm which Spencer presents solves topological sort, while the second algorithm solves breadth first search. It was mentioned in passing that the breadth first search algorithm can solve directed spanning trees and the topological sort algorithm can solve cycle detection for directed graphs. Furthermore, Spencer expands upon the breadth first search algorithm  by utilizing it to find strongly connected components and extending his breadth first search methods to support non-negative edge weights so as to solve the single source shortest paths problem.\\ 

\textbf{\textit{/* Note: We may or may not need these definitions here. */}}\\
We next reproduce three definitions given in \textit{Spencer\cite{S97}}:\\

\textit{Definition.} A vertex $v$ \textit{reaches} a vertex $u$ iff there is a path from $v$ to $u$. Conversely, $u$ is \textit{reachable} from $v$ iff there is path from $v$ to $u$.\\

\textit{Definition.} Let $G = (V, E)$ be a graph and let $R \subseteq V$. $G[R] = (R, E')$, where $E' \subseteq R \times R$.\\

\textit{Definition.} Two vertices $u,v$ are \textit{identified} by replacing them with a single vertex $uv$. The outgoing edges of both $u$ and $v$ becomes the outgoing edges of $uv$ and the incoming edges of both $u$ and $v$ become the incoming edges of $uv$.

%----------------------------------------------------------------------------------------
%	RESULTS
%----------------------------------------------------------------------------------------

\section{Results and Techniques of Assigned paper}

Let $G = (V,E)$, $m = |E|$, and $n = |V|$. The major contributions of \textit{Spencer\cite{S97}} are as follows:

\begin{itemize}
\item Algorithms that solves topological sort and breadth first search in $O(\frac{n}{\rho}\log^2{\rho})$ time with $\rho^3$ processors on an EREW PRAM such that $ \sqrt{\frac{m}{n}} \leq \rho \leq n$.
\item A randomized algorithm that solves strongly connected components in a directed graph in $O(\frac{n}{\rho}\log^2{\rho}\log{n})$ expected time and $O(n\rho^2\log^2{p}\log{n})$ expected work such that $ \sqrt{\frac{m}{n}} \leq \rho \leq n$.
\item An algorithm that solves single source shortest paths in $O(\frac{n}{\rho}\log{n}\log{(L\rho)})$ time and $O(n\rho^2(\log{n} + \log{(L\rho)}\log{\rho}) + m(\log{n} + \log{(L\rho)}))$ work such that $\log{(L\rho)} <= \rho <= n$, where $L$ is the length of the longest edge.
\end{itemize}

\subsection{Techniques}
\hfill

\textit{Definition.} The \textit{nearby list}, $NL(v)$, of a vertex $v$ consists of ordered pairs $(u, d(v,u))$ where $u$ is a vertex reachable from $v$ and $d(v,u)$ is the distance from $v$ to $u$.\\

\textit{Definition.} A vertex $v$ is \textit{terminal} if $r(v)$, the nearby radius of $v$, is infinite and $NL(v)$ contains all reachable vertices from $v$. A vertex $v$ is \textit{totally terminal} if $v$ is terminal and all vertices in $NL(v)$ is terminal.\\

In solving topological sort, \textit{Spencer\cite{S97}} actually solves the reverse topological sort which can be easily converted to a topological sort by replacing $w(v)$ with $n + 1 - w(v)$, where $w(v)$ is the label given by the reverse topological sort. \textit{Karp-Ramachandran\cite{KR90}} presented an algorithm that can compute this topological sort in $O(\log^2{n})$ time and $O(n^3\log{n})$ work by computing the transitive closure, $G^*$, of $G$. \textit{Spencer\cite{S97}} introduces an algorithm, \textit{Rtopo}, that performs less work by computing parts of $G^*$ instead of the whole thing. This is achieved by the nearby list data structure, whose size is controlled by the parameter $\rho$. The smallest $w(v)$ is assigned to the totally terminal vertices, and the nearby lists are updated to expose newly formed totally terminal vertices. The details of which we omit here.\\   

\textit{Definition.} A vertex $v$ is \textit{known} iff the distance from $s$ to $v$ is at most $R_k$, the known radius.\\

In solving breadth first search, \textit{Spencer\cite{S97}} employs a very similar algorithm to \textit{Rotpo} called \textit{pbfs}. Again, there exists a parallel algorithm that can solve breadth first search in $O(\log^2{n})$ time and $O(n^3\log{n})$ work by calculating the transitive closure $G^*$. \textit{pbfs} reduced the work by deleting vertices from $G_u$, the unknown graph consisting of unknown vertices, once it has been known. All the techniques developed for \textit{Rtopo} are applicable to \textit{bpfs}. \\ 

\textit{Definition.} A Las Vegas algorithm is a probabilistic algorithm which can detect when it has made a mistake and try again, thereby guaranteeing a correct solution.\\ 

\textit{Definition.} A vertex $v$ is \textit{big} iff it reaches more than \textit{$\frac{n}{8}$} vertices, otherwise the vertex is small.\\ 

In solving strongly connected components, Spencer again utilizes pbfs. The algorithm utilizes divide and conquer in order to separate subsets of SCC's and subgraphs of nodes which have not yet been placed in a SCC. He first applies a procedure called "split" which removes any fully explored SCC's, where he utilizes uniform randomness to select a node v in one of the subgraphs in order to compute its SCC and then separates the nodes which were reachable from v but not in its SCC by adding these nodes to a new subgraph for further processing. At this point, Spencer's algorithm utilizes the divide and conquer technique by recursively calling itself on each of the induced subgraphs until all of the SCC's have been found. Although this solution is correct with high probability, before termination, Spencer utilizes \textit{Ullman's\cite{UY91}} method of verifying the SCC's and rerunning the algorithm if the the verification fails in order to transform his algorithm into a Las Vegas Algorithm. Since the true running time of the algorithm is probabilistic, Spencer utilizes a distinction between "large" and "small" vertices along with Chebyshev's Inequality to lower bound the probability of success by a constant factor, thereby allowing him to treat the necessary number of repeated runs as a constant and absorbing this factor into the big O. With these bounds in place, the algorithm has an expected $O(\frac{n}{\rho}*log^2(\rho)*log(n))$ running time and $O(n\rho^2\log^2{\rho}\log{n})$ expected work.  \\ 

\textit{Definition.} A frontier $F(v)$ is \textit{small} iff $|F(v)|\le\rho$.\\ 

In solving Single Source Shortest Paths, Spencer employs a method very similar to pbfs, but adds additional complexity in order to account for the edge weights. The primary modifications involve modifying the nearby list invariants to be more general by increasing the range of edge weights checked before a nearby list is guaranteed to be full, introducing the idea of small nearby and frontier lists, and utilizing a safe radius whose value is determined by both the radius and estimated distance rather than simply doubling the radius each iteration. In order to manage these safe radii effectively, Spencer draws upon \textit{Paul's\cite{P83}} description of efficient operations for 2-3 trees that take $O(logn+logk)$ time, where k is the number of elements to be inserted, deleted, searched for, etc. In addition, since the longest path can now be affected by edge weights, this modification of pbfs can no longer rely upon the longest possible path being bound by $m$ and takes on a factor of L, the length of the longest edge, into multiple steps of the algorithm which ultimately impact the span and work. With these modifications, SSSP runs in  $O(\frac{n}{\rho}\log{n}\log{(L\rho)})$ time and $O(n\rho^2(\log{n} + \log{(L\rho)}\log{\rho}) + m(\log{n} + \log{(L\rho)}))$ work when $\log{(L\rho)} <= \rho <= n$.

%----------------------------------------------------------------------------------------
%	LITERATURE SURVEY
%----------------------------------------------------------------------------------------

\section{Significant Related Results}

The three most centrally related papers to \textit{Spencer\cite{S97}} are:  \textit{Karp-Ramachandran\cite{KR90}}, \textit{Ullman-Yannakakis\cite{UY91}}, and \textit{Paul\cite{P83}}.

\textit{Karp-Ramachandran\cite{KR90}} is a survey of the accumulated theory of parallel algorithms up to the 1990s. This survey focused on the PRAM as its main computational model and starts off with an overview of efficient parallel algorithms. After establishing the fundamentals, the survey explores how the different variants PRAM models handle concurrent reads and write, in addition provided a brief introduction to other parallel models different from the PRAM. Finally, the survey discussed the class of problem known as NC, problems that have parallel solutions in polylog time and perform polynomial work.  It is actually here in which \textit{Spencer\cite{S97}} makes use of \textit{Karp-Ramachandran\cite{KR90}}, where the survey listed the aforementioned six graph problems that have no known efficient solution due to the Transitive Closure Bottleneck. We believe that without \textit{Karp-Ramachandran\cite{KR90}}, there would be no \textit{Spencer\cite{S97}}, since his entire paper is stemmed from this part of the survey. There is also the fact that \textit{Spencer\cite{S97}'s Rtopo} algorithm depends on an efficient simulation of CREW on EREW to maintain the necessary data structures for its nearby lists.

\textit{Ullman-Yannakakis\cite{UY91}} introduces a high-probability parallel algorithm that finds the transitive closure from a single source in $\tilde{O}(n^\epsilon)$ time with $\tilde{O}(mn^{1-2\epsilon})$ processors such that $m \geq n^{2-3\epsilon}$. Their algorithm also finds the transitive closure for all pairs in $\tilde{O}(n^\epsilon)$ time with $\tilde{O}(mn^{1-\epsilon})$ processors such that $m \geq n^{2-2\epsilon}$. Both require $0 \leq \epsilon \leq \frac{1}{2}$. The probability that their algorithm fails to find a path is at most $2^{-c\alpha}$, where $\alpha$ is a positive constant and $c$ is some time multiplier. Their algorithm is the first in which time is sub-linear and work is less than $M(n)$. Our main interest in \textit{Ullman-Yannakakis\cite{UY91}} is the fact that they have already established an algorithm exhibiting the time-work trade off that solves breadth first search. Their algorithm is high-probability, and we were interested in seeing a different approach to the same problem we are facing in \textit{Spencer\cite{S97}}. In addition, their was extended into the context of single source shortest paths and strongly connected components, highlighting the fact that the true novelty in \textit{Spencer\cite{S97}} is it's introduction of nearby lists.

\textit{Paul\cite{P83}} introduces parallel techniques that search, insert, and delete from 2-3 trees in $O(\log{n} + \log{k})$ time on the EREW, where $n$ is the number of leaves and $k$ is the number of searches, inserts or deletes we want to perform in parallel. He seemed to be the first to design a fast tree algorithm. This results presented by \textit{Paul\cite{P83}} were used by \textit{Spencer\cite{S97}} at several key areas in his algorithms. In particular, \textit{Rtopo} needed a variation of the search algorithm to find all the totally terminal vertices to be deleted and \textit{SSSP} required a parallel extension of 2-3 trees to search, insert, and delete many items in parallel. We decided to unravel some of the details presented in Paul that Spencer decided to abstract away from his paper so that we may have a better understanding of his algorithms.


%----------------------------------------------------------------------------------------
%	PRESENTATION TOPICS
%----------------------------------------------------------------------------------------

\section{Presentation Topics}
\hfill

\textit{Lemma 3.1.2:} There are at least $min(\rho,n)$ totally terminal vertices.\\

\textit{Lemma 3.2.3:} If $NL(v)$ is eligible, then $expand(v)$ requires $O(\log{\rho})$ time and $O(\rho^2\log{\rho})$ work.\\

\textit{Lemma 3.2.6:} After the expansion process, the nearby lists satisfy their invariants.\\

\textit{Theorem 3.3.4:} The algorithm \textit{Rtopo} finds a reverse topological numbering of a directed acyclic graph in $O(\frac{n}{\rho}\log^2{\rho})$ time and does only $O(m\log{\rho} + n\rho^2\log{\rho})$ work.\\

Walter Xia will first present \textit{Rtopo} from \textit{Spencer\cite{S97}} and establish \textit{Theorem 3.3.4}. To this end, we present \textit{Lemma 3.2.3} which will be used to establish \textit{Lemma 3.2.6}. Finally, we use \textit{Lemma 3.2.6} and \textit{Lemma 3.1.2} to establish the theorem. We will avoid presenting the work bounds of \textit{Rtopo} in detail since it requires a discussion of bookkeeping details that distracts from the presentation.\\

\textit{Lemma 4.1:} The procedure \textit{explore} calculates the distance from $s$ to every vertex $v$ such that $R_k \leq d(v,s) \leq R_k + \Delta{R}$.\\

\textit{Lemma 4.2:} At least $min(\rho,n)$ vertices become known during each execution on the main loop of \textit{pbfs}.\\

\textit{Theorem 4.3:} The algorithm \textit{pbfs} computes the distance from $s$ to every other vertex in $O(\frac{n}{\rho}\log^2{\rho})$ time and does $O(m\log{p} + n\rho^2\log^2{\rho})$ work.\\

If time permits, Walter Xia will also present \textit{pbfs} from \textit{Spencer\cite{S97}} and establish \textit{Theorem 3.4}. To this end, we will present \textit{Lemma 4.1} and \textit{4.2}. This algorithm is not as critical since it's techniques are very similar to that of \textit{Rtopo}.\\

10 : Definition 3.2.4. The rank of a vertex v is log2(r(v)).\\

Matthew Hudson will first give a high level presentation on \textit{Spencer's\cite{S97}} algorithm for finding Strongly Connected Components in order and how he utilizes \textit{Ullman's\cite{UY91}} Las Vegas Verifier in order to guarantee a correct solution.\\

Lemma 6.1.1\\

Lemma 6.1.2\\

Lemma 6.1.3\\

Lemma 6.2.1\\

Lemma 6.2.2\\

Theorem 6.3.1\\

Afterwards, Matthew Hudson will present \textit{Spencer's\cite{S97}} algorithm for Single Source Shortest Paths and establish Theorem 6.3.1. In order to do so, we will present Lemmas 6.1.1-6.1.3 as well as Lemma 6.2.2. If time permits, we will also discuss \textit{Spencer's\cite{S97}} method for finding eligible vertices, however, the methods involved already appeared in Rtopo and pbfs in more basic forms, so this section is not critical for the presentation.

%----------------------------------------------------------------------------------------
%	FURTHER RESEARCH
%----------------------------------------------------------------------------------------

\section{Further Research Directions}
\textbf{\textit{/* Note: We may want to add to this. */}}\\
\textit{Spencer\cite{S97}} and \textit{Ullman\cite{UY91}} suggested that the existence of deterministic rather than probabilistic parallel algorithms that can solve strongly connected components and single source shortest paths remains an open question. \textit{Karp-Ramachandran\cite{KR90}} gave a list of open problems which Spencer did not approach that we currently do not know how to solve efficiently in parallel and have not been able to categorize into a "Complete" category, which included directed and  undirected depth first search. We believe the real interesting question is whether we can forego transitive closure as a subroutine when dealing with problems involving graphs while maintaining time optimality.



%\clearpage
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{abbrv}
\bibliography{spencer,karp_ramachandran,ullman-yannakakis,paul}

\end{document}