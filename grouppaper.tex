\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[document]{ragged2e}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\usepackage{fancyhdr} % Custom headers and footers

\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{project for CS388P: Parallel Algorithms (Fall 2016)} 
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Prioritizing Processor Efficiency Over Time Efficiency \\ % Write your project title here
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Matthew Hudson \hspace{5mm} Walter Xia }	%Write the name of all your team members here

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------
\justify
\textbf{Abstract}

\begin{abstract}
Our paper is \textit{Time-Work Tradeoffs for Parallel Algorithms}, \textit{Spencer\cite{S97}}. In this paper, Spencer focuses on several open parallel problems that are affected by the transitive closure bottleneck. For each of the below problems, Spencer describes an algorithm that comes closer to work efficiency than existing approaches that utilize transitive closure. However, this increased work efficiency comes at the cost of an asymptotic increase in the span of each algorithm relative to the time optimal solution. Spencer's motivation for prioritizing work efficiency over time efficiency stems from the fact that existing transitive closure solutions to these problems perform significantly worse than their sequential counterparts when run on only a single processor.

\underline{Problems}
\begin{itemize}
\item Solving Triangular Systems of Linear Equations
\item Topological Sort
\item Breadth-First Search in Directed Graphs
\item Strongly Connected Components
\item Single Source Shortest Path
\end{itemize}

Rather than using a matrix based approach, Spencer utilizes a new data structure called \textit{nearby lists} that serves as the work horse for the subsequent algorithms. \\
\end{abstract}

%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

\textit{Spencer\cite{S97}} focuses on the problems for which there is no known efficient parallel algorithm that solves them. That is, there is no known parallel algorithm that is able to solve them in work that is within a logarithmic factor of the fastest sequential algorithm, where work is the product of the running time and the processor count. \textit{Spencer\cite{S97}} develops parallel algorithms that solves the following problems presented in \textit{Karp-Ramachandran\cite{KR90}}:

\begin{itemize}
\item Directed Spanning Tree
\item Breadth-First Search
\item Topological Sort
\item Cycle Detection for Directed Graphs
\item Strong Connected Components
\item Single Source Shortest Paths with Non-Negative Edges
\end{itemize}

His algorithms exhibit a time-work trade-off, that is, the longer they run, the less work they perform. This phenomenon exists due of the fact that certain speculative computations can be eliminated as time progresses, thus reducing work. One algorithm solves topological sort and the other algorithm solves breadth first search. It was mentioned in passing that the breadth first search algorithm can solve directed spanning trees and the topological sort algorithm can solve cycle detection for directed graphs. Furthermore, the breadth first search algorithm can be applied to strongly connected components and extended to single source shortest paths with non-negative edges.\\

\textbf{\textit{/* Note: We may or may not need these definitions here. */}}\\
We next reproduce three definitions given in \textit{Spencer\cite{S97}}:\\

\textit{Definition.} A vertex $v$ \textit{reaches} a vertex $u$ iff there is a path from $v$ to $u$. Conversely, $u$ is \textit{reachable} from $v$ iff there is path from $v$ to $u$.\\

\textit{Definition.} Let $G = (V, E)$ be a graph and let $R \subseteq V$. $G[R] = (R, E')$, where $E' \subseteq R \times R$.\\

\textit{Definition.} Two vertices $u,v$ are \textit{identified} by replacing them with a single vertex $uv$. The outgoing edges of both $u$ and $v$ becomes the outgoing edges of $uv$ and the incoming edges of both $u$ and $v$ become the incoming edges of $uv$.

%----------------------------------------------------------------------------------------
%	RESULTS
%----------------------------------------------------------------------------------------

\section{Results and Techniques of Assigned paper}

Let $G = (V,E)$, $m = |E|$, and $n = |V|$. The major contributions of \textit{Spencer\cite{S97}} are as follows:

\begin{itemize}
\item Algorithms that solves topological sort and breadth first search in $O(\frac{n}{\rho}\log^2{\rho})$ time with $\rho^3$ processors on an EREW PRAM such that $ \sqrt{\frac{m}{n}} \leq \rho \leq n$.
\item A randomized algorithm that solves strongly connected components in a directed graph in $O(\frac{n}{\rho}\log^2{\rho}\log{n})$ expected time and $O(n\rho^2\log^2{p}\log{n})$ expected work such that $ \sqrt{\frac{m}{n}} \leq \rho \leq n$.
\item An algorithm that solves single source shortest paths in $O(\frac{n}{\rho}\log{n}\log{(L\rho)})$ time and $O(n\rho^2(\log{n} + \log{(L\rho)}\log{\rho}) + m(\log{n} + \log{(L\rho)}))$ work such that $\log{(L\rho)} <= \rho <= n$, where $L$ is the length of the longest edge.
\end{itemize}

\subsection{Techniques}
\hfill

\textit{Definition.} The \textit{nearby list}, $NL(v)$, of a vertex $v$ consists of ordered pairs $(u, d(v,u))$ where $u$ is a vertex reachable from $v$ and $d(v,u)$ is the distance from $v$ to $u$.\\

\textit{Definition.} A vertex $v$ is \textit{terminal} if $r(v)$, the nearby radius of $v$, is infinite and $NL(v)$ contains all reachable vertices from $v$. A vertex $v$ is \textit{totally terminal} if $v$ is terminal and all vertices in $NL(v)$ is terminal.\\

In solving topological sort, \textit{Spencer\cite{S97}} actually solves the reverse topological sort which can be easily converted to a topological sort by replacing $w(v)$ with $n + 1 - w(v)$, where $w(v)$ is the label given by the reverse topological sort. \textit{Karp-Ramachandran\cite{KR90}} presented an algorithm that can compute this topological sort in $O(\log^2{n})$ time and $O(n^3\log{n})$ work by computing the transitive closure, $G^*$, of $G$. \textit{Spencer\cite{S97}} introduces an algorithm, \textit{Rtopo}, that performs less work by computing parts of $G^*$ instead of the whole thing. This is achieved by the nearby list data structure, whose size is controlled by the parameter $\rho$. The smallest $w(v)$ is assigned to the totally terminal vertices, and the nearby lists are updated to expose newly formed totally terminal vertices. The details of which we omit here.\\   

\textit{Definition.} A vertex $v$ is \textit{known} iff the distance from $s$ to $v$ is at most $R_k$, the known radius.\\

In solving breadth first search, \textit{Spencer\cite{S97}} employs a very similar algorithm to \textit{Rotpo} called \textit{pbfs}. Again, there exists a parallel algorithm that can solve breadth first search in $O(\log^2{n})$ time and $O(n^3\log{n})$ work by calculating the transitive closure $G^*$. \textit{pbfs} reduced the work by deleting vertices from $G_u$, the unknown graph consisting of unknown vertices, once it has been known. All the techniques developed for \textit{Rtopo} are applicable to \textit{bpfs}. 

%----------------------------------------------------------------------------------------
%	LITERATURE SURVEY
%----------------------------------------------------------------------------------------

\section{Significant Related Results}

The three most centrally related papers to Spencer we chose are: Karp-Ramachandran, Ullman-Yannakakis, and Paul.

Karp-Ramachandran is a survey of the accumlated theory of parallel algorithms up to the 1990s. This survey focused on the PRAM as its main computational model and starts off with an overview of efficient parallel algorithms on arrays, algebraic expressions, graphs, sorting, etc... After estabishing the fundamentals, the survey explores the different vairants PRAM models handle concurrent reads and write, in addition provided a brief introduction to other parallel models different from the PRAM. Finally, the survey discussed the class of problem known as NC, problems that have parallel solutions in polylog time and perform polynomial work.  It is actually here in which Spencer makes use of Karp-Ramachandran, where the survey listed the aformentioned six graph problems that have no known efficient solution due to the Transitive Closure Bottleneck. We believe that without Karp-Ramachandran, there would be no Spencer, since his entire paper is stemmed from this part of the survey. There is also the fact that Spencer's Rtopo's algorithm depends on an efficient simulation of CREW on EREW to maintain the necessary data structures for its nearby lists.

Ullman-Yannakakis introduces a high-probablilty parallel algorithm that finds the transitive closure from a single source in ~O(n^e') time with ~O(mn^(1-2e')) processors such that e >= n^(2-3e'). Their algorithm also finds the transitive closure for all pairs in ~O(n^e') time with ~O(mn^(1-e')) processors such that m >= n^(2-2e'). They require 0 <= e' <= 1/2. The probability that their algorithm fails to find a path is at most 2^(-ac), where a is a positive constant and c is some time multiplier. Their algorithm is the first in which time is sublinear and work is less than M(n). Our main interest in Ullman-Yannakakis is the fact that they have already established an algorithm exhibiting the time-work tradeoff that solves breadth first search. Their algorithm is high-probability, and we were interested in seeing a different approach to the same problem we are facing in Spencer. In addition, their was extended into the context of single source shortest paths and strongly connected components, highlighting the fact that the true novelty in Spencer is it's introduction of nearby lists.

Paul introduces parallel techniques than search, insert, and delete from 2-3 treens in O(logn + logk) time on the EREW, where n is the number of leaves and k is the number of searches, inserts or deletes we want to perform in parallel. He seemed to be the first to design a fast tree algorithm. This results presented by Paul were used by Spencer at several key areas in his algorithms. In particular, Rtopo needed a variation of the search algorithm to find all the totally terminal vertices to be deleted and SSSP required a parallel extention of 2-3 trees to search, insert, and delete many items in parallel. We decided to unpeel some of the details presented in Paul that Spencer decided to abtract away from his paper so that we may have a better understanding of his algorithms.


%----------------------------------------------------------------------------------------
%	PRESENTATION TOPICS
%----------------------------------------------------------------------------------------

\section{Presentation Topics}

Here each team member should cite the paper from which they will present technical results in their
presentation, and should indicate the exact theorem(s)/lemma(s) whose proof(s) they will present.

%----------------------------------------------------------------------------------------
%	FURTHER RESEARCH
%----------------------------------------------------------------------------------------

\section{Further Research Directions}
Give an overview of the current open problems and future research directions related to the work in the assigned paper.



%\clearpage
%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\bibliographystyle{abbrv}
\bibliography{spencer,karp_ramachandran}

\end{document}